{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation:\n",
    "\n",
    "Data obtained from the MNIST database website (LeCun, Cortes, Burges) [here](http://yann.lecun.com/exdb/mnist/), downloaded locally through browser. \n",
    "\n",
    "The files are in IDX form, see the bottom of the linked page for more information.  We use the idx2numpy package to handle the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: If you are using Anaconda and you used PIP install in the prompt to get the pacakge, \n",
    "# you need to make sure it's installed to the correct environment. For example, use \n",
    "# \"activate py36\" in the prompt before installing the package.\n",
    "\n",
    "try:\n",
    "    import idx2numpy\n",
    "except:\n",
    "    print(\"Package not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the IDX file in as a numpy array. I have the data saved in a folder labeled MNIST.\n",
    "\n",
    "train_images = idx2numpy.convert_from_file('MNIST//train-images.idx3-ubyte')\n",
    "train_labels = idx2numpy.convert_from_file('MNIST//train-labels.idx1-ubyte')\n",
    "test_images = idx2numpy.convert_from_file('MNIST//t10k-images.idx3-ubyte')\n",
    "test_labels = idx2numpy.convert_from_file('MNIST//t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to check that the data makes sense. \n",
    "\n",
    "The image data is stored as what could be described as an a array of matrices, so that for example, train_images\\[0\\] is a $28\\times 28$ matrix of grey-scale integer pixel values (0=white, 255 = black). The pyplot imshow function will display such a matrix as an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_image(image_array, label_array):\n",
    "    num_images = len(image_array)\n",
    "    random_int = random.randint(0,num_images-1)\n",
    "    plt.imshow(image_array[random_int], cmap = 'Greys')\n",
    "    print(\"Index: \" + str(random_int))\n",
    "    print(\"Label: \" + str(label_array[random_int]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 49057\n",
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_image(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 4160\n",
      "Label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_image(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for data balance:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of classes: Counter({1: 6742, 7: 6265, 3: 6131, 2: 5958, 9: 5949, 0: 5923, 6: 5918, 8: 5851, 4: 5842, 5: 5421})\n"
     ]
    }
   ],
   "source": [
    "train_class_count = Counter()\n",
    "for label in train_labels:\n",
    "    train_class_count[label] += 1\n",
    "    \n",
    "print(\"Instance of classes: \", end = '')\n",
    "print(train_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of classes: Counter({1: 1135, 2: 1032, 7: 1028, 3: 1010, 9: 1009, 4: 982, 0: 980, 8: 974, 6: 958, 5: 892})\n"
     ]
    }
   ],
   "source": [
    "test_class_count = Counter()\n",
    "for label in test_labels:\n",
    "    test_class_count[label] += 1\n",
    "    \n",
    "print(\"Instance of classes: \", end = '')\n",
    "print(test_class_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a balanced training/validation set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(data_array, label_array, train_ratio):\n",
    "    labels = np.unique(label_array) # An array of the distinct entry values occuring in the argument.\n",
    "    train_indices = []\n",
    "    valid_indices = []\n",
    "    for label in labels:\n",
    "        label_indices = [i for i,x in enumerate(label_array) if x == label]\n",
    "        instance_count = len(label_indices)\n",
    "        partition = int(math.floor(train_ratio*instance_count))\n",
    "        random.shuffle(label_indices)\n",
    "        to_train = label_indices[:partition]\n",
    "        to_valid = label_indices[partition:]\n",
    "        train_indices = train_indices + to_train\n",
    "        valid_indices = valid_indices + to_valid\n",
    "    random.shuffle(train_indices)\n",
    "    random.shuffle(valid_indices)\n",
    "    train_data = data_array[train_indices]\n",
    "    train_labels = label_array[train_indices]\n",
    "    valid_data = data_array[valid_indices]\n",
    "    valid_labels = label_array[valid_indices]\n",
    "    return train_data, train_labels, valid_data, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size: 47995\n",
      "Validation Size: 12005\n",
      "Instance of classes: Counter({1: 5393, 7: 5012, 3: 4904, 2: 4766, 9: 4759, 0: 4738, 6: 4734, 8: 4680, 4: 4673, 5: 4336})\n",
      "Instance of classes: Counter({1: 1349, 7: 1253, 3: 1227, 2: 1192, 9: 1190, 0: 1185, 6: 1184, 8: 1171, 4: 1169, 5: 1085})\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, valid_images, valid_labels = train_validation_split(train_images, train_labels, .8)\n",
    "\n",
    "print(\"Test Size: \" + str(len(train_images)))\n",
    "print(\"Validation Size: \" + str(len(valid_images)))\n",
    "\n",
    "train_class_count = Counter()\n",
    "for label in train_labels:\n",
    "    train_class_count[label] += 1\n",
    "    \n",
    "print(\"Instance of classes: \", end = '')\n",
    "print(train_class_count)\n",
    "\n",
    "valid_class_count = Counter()\n",
    "for label in valid_labels:\n",
    "    valid_class_count[label] += 1\n",
    "    \n",
    "print(\"Instance of classes: \", end = '')\n",
    "print(valid_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 32890\n",
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_image(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 10856\n",
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_image(valid_images, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data as a 2 dimensional array for matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images, normalize them between -1 and 1, and one-hot encode the labels. \n",
    "\n",
    "def reformat(images, labels):\n",
    "    out_images = images.reshape((-1,28*28)).astype(np.float32)\n",
    "    out_images = (out_images-(255.)/2)/255.\n",
    "    out_labels = (np.arange(10) == labels[:,None]).astype(np.float32)\n",
    "    return out_images, out_labels\n",
    "\n",
    "# Unflatten images. Need: unencode labels [gracefully - probably np.where(array = 1) somehow].\n",
    "\n",
    "def unreformat(images):\n",
    "    out_images = images.reshape((-1,28,28)).astype(np.float32)\n",
    "    return out_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = reformat(train_images, train_labels)\n",
    "valid_images, valid_labels = reformat(valid_images, valid_labels)\n",
    "test_images, test_labels = reformat(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (47995, 784)\n",
      "Validation Data Shape: (12005, 784)\n",
      "Test Data Shape: (10000, 784)\n",
      "Training Labels Shape: (47995, 10)\n",
      "Validation Labels Shape: (12005, 10)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Shape: \" + str(train_images.shape))\n",
    "print(\"Validation Data Shape: \" + str(valid_images.shape))\n",
    "print(\"Test Data Shape: \" + str(test_images.shape))\n",
    "\n",
    "print(\"Training Labels Shape: \" + str(train_labels.shape))\n",
    "print(\"Validation Labels Shape: \" + str(valid_labels.shape))\n",
    "print(\"Test Labels Shape: \" + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Constructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_labels, true_labels):\n",
    "    return(100*np.sum(np.argmax(predicted_labels, 1)==np.argmax(true_labels, 1))\n",
    "           /predicted_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-0621d1647c83>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regression_graph = tf.Graph()\n",
    "\n",
    "with regression_graph.as_default():\n",
    "    # Feed in the constant data.\n",
    "    T = tf.constant(train_images)\n",
    "    Tlabel = tf.constant(train_labels)\n",
    "    V = tf.constant(valid_images)\n",
    "    E = tf.constant(test_images)\n",
    "    # Establish variable matrix and bias vector.\n",
    "    W = tf.Variable(tf.truncated_normal([784, 10])) # Randomly initialized.\n",
    "    b = tf.Variable(tf.zeros([10])) # Initialized at origin. \n",
    "    # Do the logit computation.\n",
    "    L = tf.matmul(T,W)+b\n",
    "    # Do the softmax and crossentropy.\n",
    "    l = tf.nn.softmax_cross_entropy_with_logits(labels = Tlabel, logits = L)\n",
    "    # Take the average to get the loss function output.\n",
    "    loss = tf.reduce_mean(l)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    # Monitoring outputs.\n",
    "    train_predictions = tf.nn.softmax(L)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(V,W)+b)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(E,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n",
      "Iteration 0: loss = 14.652564,  training accuracy = 9.7%,  validation accuracy = 9.7%.\n",
      "Iteration 100: loss = 1.189335,  training accuracy = 75.3%,  validation accuracy = 75.2%.\n",
      "Iteration 200: loss = 0.861220,  training accuracy = 81.2%,  validation accuracy = 81.3%.\n",
      "Iteration 300: loss = 0.726796,  training accuracy = 83.7%,  validation accuracy = 83.5%.\n",
      "Iteration 400: loss = 0.649219,  training accuracy = 85.1%,  validation accuracy = 84.8%.\n",
      "Iteration 500: loss = 0.597067,  training accuracy = 86.0%,  validation accuracy = 85.6%.\n",
      "Iteration 600: loss = 0.558825,  training accuracy = 86.6%,  validation accuracy = 86.2%.\n",
      "Iteration 700: loss = 0.529230,  training accuracy = 87.2%,  validation accuracy = 86.7%.\n",
      "Iteration 800: loss = 0.505484,  training accuracy = 87.6%,  validation accuracy = 87.2%.\n",
      "Iteration 900: loss = 0.485915,  training accuracy = 88.0%,  validation accuracy = 87.5%.\n",
      "Test accuracy: 88.4%\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations = 1000\n",
    "\n",
    "with tf.Session(graph = regression_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Variables Initialized')\n",
    "    for iteration in range(number_of_iterations):\n",
    "        session.run(optimizer)\n",
    "        if (iteration % 100 == 0):\n",
    "            print('Iteration %d: ' % iteration, end=\"\")\n",
    "            l = loss.eval()\n",
    "            print('loss = %f, ' % l, end=\"\")\n",
    "            train_predict = train_predictions.eval()\n",
    "            print(' training accuracy = %.1f%%' % accuracy(train_predict, train_labels), end=\", \")\n",
    "            valid_predict = valid_predictions.eval()\n",
    "            print(' validation accuracy = %.1f%%.' % accuracy(valid_predict, valid_labels))\n",
    "    test_predict = test_predictions.eval()\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_predict, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Regression\n",
    "\n",
    "Train the weights against small batches instead of the whole dataset at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_regression_graph = tf.Graph()\n",
    "batch_size = 1000\n",
    "\n",
    "with stochastic_regression_graph.as_default():\n",
    "    # Placeholders to feed batches of training data.\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_size, 784)) # Trains batches of 100.\n",
    "    Tlabel = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    # Validation and test data remain constant.\n",
    "    V = tf.constant(valid_images)\n",
    "    E = tf.constant(test_images)\n",
    "    # Establish variable matrix and bias vector.\n",
    "    W = tf.Variable(tf.truncated_normal([784, 10])) # Randomly initialized.\n",
    "    b = tf.Variable(tf.zeros([10])) # Initialized at origin. \n",
    "    # logit computation\n",
    "    L = tf.matmul(T,W)+b\n",
    "    # softmax, crossentropy and loss\n",
    "    smce = tf.nn.softmax_cross_entropy_with_logits(labels = Tlabel, logits = L)\n",
    "    loss = tf.reduce_mean(smce)\n",
    "    # Optimize\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    # Monitoring outputs.\n",
    "    train_predictions = tf.nn.softmax(L)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(V,W)+b)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(E,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized.\n",
      "Epoch 0, Iteration: 1\n",
      "loss = 13.845592, training accuracy = 9.3%, validation accuracy = 15.3%.\n",
      "Epoch 100, Iteration: 4801\n",
      "loss = 0.283729, training accuracy = 92.1%, validation accuracy = 90.6%.\n",
      "Epoch 200, Iteration: 9601\n",
      "loss = 0.268764, training accuracy = 92.6%, validation accuracy = 90.9%.\n",
      "Epoch 300, Iteration: 14400\n",
      "loss = 0.301649, training accuracy = 92.5%, validation accuracy = 91.2%.\n",
      "Epoch 400, Iteration: 19200\n",
      "loss = 0.191351, training accuracy = 94.4%, validation accuracy = 91.5%.\n",
      "Epoch 500, Iteration: 23999\n",
      "loss = 0.255783, training accuracy = 93.3%, validation accuracy = 91.4%.\n",
      "Epoch 600, Iteration: 28799\n",
      "loss = 0.261805, training accuracy = 93.2%, validation accuracy = 91.7%.\n",
      "Epoch 700, Iteration: 33599\n",
      "loss = 0.234398, training accuracy = 93.6%, validation accuracy = 91.7%.\n",
      "Epoch 800, Iteration: 38398\n",
      "loss = 0.172036, training accuracy = 95.1%, validation accuracy = 91.5%.\n",
      "Epoch 900, Iteration: 43198\n",
      "loss = 0.229250, training accuracy = 93.8%, validation accuracy = 91.5%.\n",
      "Epoch 1000, Iteration: 47997\n",
      "loss = 0.214826, training accuracy = 93.1%, validation accuracy = 91.5%.\n",
      "Test accuracy: 92.3%\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 1000\n",
    "\n",
    "with tf.Session(graph = stochastic_regression_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    epoch = 0\n",
    "    print('Variables Initialized.')\n",
    "    last_offset = 0\n",
    "    iteration = 0\n",
    "    inner_epoch_samples = 0\n",
    "    last_epoch = -1\n",
    "    while epoch <= number_of_epochs+1:\n",
    "        # Generate Batches\n",
    "        offset = (iteration * batch_size) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_images[offset :(offset + batch_size), : ]\n",
    "        batch_labels = train_labels[offset : (offset + batch_size), : ]\n",
    "        \n",
    "        # Randomize Within Batch: This slows things down but may be better pratice.\n",
    "        randomized_indices = list(range(batch_size))\n",
    "        random.shuffle(randomized_indices)\n",
    "        randomized_batch_data = batch_data[randomized_indices]\n",
    "        randomized_batch_labels = batch_labels[randomized_indices]\n",
    "        \n",
    "        # Run Model\n",
    "        feed_dict = {T : randomized_batch_data, Tlabel : randomized_batch_labels}\n",
    "        _, l, train_predict = session.run([optimizer, loss, train_predictions], feed_dict = feed_dict)\n",
    "        iteration += 1\n",
    "        inner_epoch_samples += batch_size\n",
    "        \n",
    "        # Report\n",
    "        if (epoch % 100) == 0 and epoch > last_epoch:\n",
    "            print('Epoch %d, Iteration: %d' % (epoch, iteration))\n",
    "            print('loss = %f' % l, end = \", \")\n",
    "            train_acc = accuracy(train_predict, randomized_batch_labels)\n",
    "            print('training accuracy = %.1f%%' % train_acc, end = \", \")\n",
    "            valid_acc = accuracy(valid_predictions.eval(), valid_labels)\n",
    "            print('validation accuracy = %.1f%%.' % valid_acc)\n",
    "            last_epoch = epoch\n",
    "        if inner_epoch_samples - train_labels.shape[0] > 0:\n",
    "            inner_epoch_samples = inner_epoch_samples - (train_labels.shape[0]+1)\n",
    "            epoch += 1\n",
    "    test_predict = test_predictions.eval()\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_predict, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network, 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "number_of_nodes = 1024 # Note: Results in a bigger network, so decrease learning rate. \n",
    "\n",
    "FCNN_1HL_graph = tf.Graph()\n",
    "\n",
    "def FCNN_1HL_model(data, dropout = True):\n",
    "        log1 = tf.matmul(data, W1) + b1\n",
    "        hid1 = tf.nn.leaky_relu(log1)\n",
    "        if dropout:\n",
    "            hid1 = tf.nn.dropout(hid1, keep_prob = .5)\n",
    "        log2 = tf.matmul(hid1, W2)+b2\n",
    "        return log2\n",
    "\n",
    "with FCNN_1HL_graph.as_default():\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_size, 784)) # Trains batches of 100.\n",
    "    Tlabel = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    # Validation and test data remain constant.\n",
    "    V = tf.constant(valid_images)\n",
    "    E = tf.constant(test_images)\n",
    "    # Establish variable matrices and bias vectors.\n",
    "    W1 = tf.Variable(tf.random_normal([784, number_of_nodes]))\n",
    "    b1 = tf.Variable(tf.zeros([number_of_nodes]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([number_of_nodes, 10]))\n",
    "    b2 = tf.Variable(tf.zeros([10])) \n",
    "    # Model\n",
    "    L = FCNN_1HL_model(T)\n",
    "    # logits, softmax, crossentropy and loss\n",
    "    smce = tf.nn.softmax_cross_entropy_with_logits(labels = Tlabel, logits = L)\n",
    "    loss = tf.reduce_mean(smce)\n",
    "    # Optimize\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    # Monitoring outputs.\n",
    "    train_predictions = tf.nn.softmax(FCNN_1HL_model(T, dropout = False))\n",
    "    valid_predictions = tf.nn.softmax(FCNN_1HL_model(V, dropout = False))\n",
    "    test_predictions = tf.nn.softmax(FCNN_1HL_model(E, dropout = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized.\n",
      "Epoch 0, Iteration: 1\n",
      "loss = 649.817566, training accuracy = 10.0%, validation accuracy = 12.2%.\n",
      "Epoch 2, Iteration: 961\n",
      "loss = 15.403095, training accuracy = 91.0%, validation accuracy = 91.8%.\n",
      "Epoch 4, Iteration: 1921\n",
      "loss = 15.983130, training accuracy = 92.0%, validation accuracy = 93.3%.\n",
      "Epoch 6, Iteration: 2881\n",
      "loss = 5.002710, training accuracy = 99.0%, validation accuracy = 94.0%.\n",
      "Epoch 8, Iteration: 3841\n",
      "loss = 3.689040, training accuracy = 94.0%, validation accuracy = 94.4%.\n",
      "Epoch 10, Iteration: 4801\n",
      "loss = 3.825443, training accuracy = 96.0%, validation accuracy = 94.8%.\n",
      "Epoch 12, Iteration: 5761\n",
      "loss = 3.221264, training accuracy = 94.0%, validation accuracy = 94.9%.\n",
      "Epoch 14, Iteration: 6721\n",
      "loss = 4.457300, training accuracy = 95.0%, validation accuracy = 95.1%.\n",
      "Epoch 16, Iteration: 7681\n",
      "loss = 0.806422, training accuracy = 96.0%, validation accuracy = 95.4%.\n",
      "Epoch 18, Iteration: 8641\n",
      "loss = 1.326221, training accuracy = 96.0%, validation accuracy = 95.2%.\n",
      "Epoch 20, Iteration: 9601\n",
      "loss = 2.105155, training accuracy = 98.0%, validation accuracy = 95.5%.\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 20\n",
    "\n",
    "with tf.Session(graph = FCNN_1HL_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    epoch = 0\n",
    "    print('Variables Initialized.')\n",
    "    last_offset = 0\n",
    "    iteration = 0\n",
    "    inner_epoch_samples = 0\n",
    "    last_epoch = -1\n",
    "    while epoch <= number_of_epochs+1:\n",
    "        # Generate Batches\n",
    "        offset = (iteration * batch_size) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_images[offset :(offset + batch_size), : ]\n",
    "        batch_labels = train_labels[offset : (offset + batch_size), : ]\n",
    "        \n",
    "        # Randomize Within Batch: This slows things down but may be better pratice in general.\n",
    "        randomized_indices = list(range(batch_size))\n",
    "        random.shuffle(randomized_indices)\n",
    "        randomized_batch_data = batch_data[randomized_indices]\n",
    "        randomized_batch_labels = batch_labels[randomized_indices]\n",
    "    \n",
    "        # Run Model\n",
    "        feed_dict = {T : randomized_batch_data, Tlabel : randomized_batch_labels}\n",
    "        _, l, train_predict = session.run([optimizer, loss, train_predictions], feed_dict = feed_dict)\n",
    "        iteration += 1\n",
    "        inner_epoch_samples += batch_size\n",
    "        \n",
    "        # Report\n",
    "        if (epoch % 2) == 0 and epoch > last_epoch:\n",
    "            print('Epoch %d, Iteration: %d' % (epoch, iteration))\n",
    "            print('loss = %f' % l, end = \", \")\n",
    "            train_acc = accuracy(train_predict, randomized_batch_labels)\n",
    "            print('training accuracy = %.1f%%' % train_acc, end = \", \")\n",
    "            valid_acc = accuracy(valid_predictions.eval(), valid_labels)\n",
    "            print('validation accuracy = %.1f%%.' % valid_acc)\n",
    "            last_epoch = epoch\n",
    "        if inner_epoch_samples - train_labels.shape[0] > 0:\n",
    "            inner_epoch_samples = inner_epoch_samples - (train_labels.shape[0]+1)\n",
    "            epoch += 1\n",
    "    test_predict = test_predictions.eval()\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_predict, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network, 4 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "entry_number_of_nodes = 1024 # Note: Results in a MUCH bigger network with divisions, \n",
    "                        #so decrease learning rate/initialization standard deviation. \n",
    "\n",
    "learning_rate = .025\n",
    "init_stddev = .05\n",
    "\n",
    "FCNN_4HL_graph = tf.Graph()\n",
    "\n",
    "\n",
    "\n",
    "with FCNN_4HL_graph.as_default():\n",
    "    # Placeholder for batch data.\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_size, 784))\n",
    "    Tlabel = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    # Validation and test data remain constant.\n",
    "    V = tf.constant(valid_images)\n",
    "    E = tf.constant(test_images)\n",
    "    # Establish variable matrices and bias vectors.\n",
    "    W1 = tf.Variable(tf.random_normal([784, number_of_nodes], stddev = init_stddev))\n",
    "    b1 = tf.Variable(tf.zeros([number_of_nodes]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([number_of_nodes, int(number_of_nodes/2)], stddev = init_stddev))\n",
    "    b2 = tf.Variable(tf.zeros([int(number_of_nodes/2)])) \n",
    "    W3 = tf.Variable(tf.truncated_normal([int(number_of_nodes/2), int(number_of_nodes/4)], stddev = init_stddev))\n",
    "    b3 = tf.Variable(tf.zeros([int(number_of_nodes/4)])) \n",
    "    W4 = tf.Variable(tf.truncated_normal([int(number_of_nodes/4), int(number_of_nodes/8)], stddev = init_stddev))\n",
    "    b4 = tf.Variable(tf.zeros([int(number_of_nodes/8)])) \n",
    "    W5 = tf.Variable(tf.truncated_normal([int(number_of_nodes/8), 10], stddev = init_stddev))\n",
    "    b5 = tf.Variable(tf.zeros([10])) \n",
    "    # Model\n",
    "    def FCNN_4HL_model(data, dropout = True):\n",
    "        log1 = tf.matmul(data, W1) + b1\n",
    "        hid1 = tf.nn.leaky_relu(log1)\n",
    "        if dropout:\n",
    "            hid1 = tf.nn.dropout(hid1, keep_prob = .5)\n",
    "        log2 = tf.matmul(hid1, W2)+b2\n",
    "        hid2 = tf.nn.leaky_relu(log2)\n",
    "        if dropout:\n",
    "            hid2 = tf.nn.dropout(hid2, keep_prob = .5)\n",
    "        log3 = tf.matmul(hid2, W3)+b3\n",
    "        hid3 = tf.nn.leaky_relu(log3)\n",
    "        if dropout:\n",
    "            hid3 = tf.nn.dropout(hid3, keep_prob = .5)\n",
    "        log4 = tf.matmul(hid3, W4)+b4\n",
    "        hid4 = tf.nn.leaky_relu(log4)\n",
    "        if dropout:\n",
    "            hid4 = tf.nn.dropout(hid4, keep_prob = .5)\n",
    "        log5 = tf.matmul(hid4, W5)+b5\n",
    "        return log5\n",
    "    \n",
    "    \n",
    "    # logits, softmax, crossentropy and loss\n",
    "    L = FCNN_4HL_model(T)\n",
    "    smce = tf.nn.softmax_cross_entropy_with_logits(labels = Tlabel, logits = L)\n",
    "    loss = tf.reduce_mean(smce)\n",
    "    # Optimize\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    # Monitoring outputs.\n",
    "    train_predictions = tf.nn.softmax(FCNN_4HL_model(T, dropout = False))\n",
    "    valid_predictions = tf.nn.softmax(FCNN_4HL_model(V, dropout = False))\n",
    "    test_predictions = tf.nn.softmax(FCNN_4HL_model(E, dropout = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized.\n",
      "Epoch 0, Iteration: 1\n",
      "loss = 2.386708, training accuracy = 13.0%, validation accuracy = 12.3%.\n",
      "Epoch 2, Iteration: 961\n",
      "loss = 0.765348, training accuracy = 84.0%, validation accuracy = 87.0%.\n",
      "Epoch 4, Iteration: 1921\n",
      "loss = 0.432627, training accuracy = 92.0%, validation accuracy = 90.4%.\n",
      "Epoch 6, Iteration: 2881\n",
      "loss = 0.291401, training accuracy = 90.0%, validation accuracy = 92.1%.\n",
      "Epoch 8, Iteration: 3841\n",
      "loss = 0.306259, training accuracy = 95.0%, validation accuracy = 93.3%.\n",
      "Epoch 10, Iteration: 4801\n",
      "loss = 0.269199, training accuracy = 95.0%, validation accuracy = 94.0%.\n",
      "Epoch 12, Iteration: 5761\n",
      "loss = 0.225274, training accuracy = 93.0%, validation accuracy = 94.3%.\n",
      "Epoch 14, Iteration: 6721\n",
      "loss = 0.298702, training accuracy = 94.0%, validation accuracy = 94.8%.\n",
      "Epoch 16, Iteration: 7681\n",
      "loss = 0.218531, training accuracy = 96.0%, validation accuracy = 95.2%.\n",
      "Epoch 18, Iteration: 8641\n",
      "loss = 0.164751, training accuracy = 98.0%, validation accuracy = 95.3%.\n",
      "Epoch 20, Iteration: 9601\n",
      "loss = 0.145662, training accuracy = 98.0%, validation accuracy = 95.5%.\n",
      "Test accuracy: 96.2%\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 20\n",
    "\n",
    "with tf.Session(graph = FCNN_4HL_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    epoch = 0\n",
    "    print('Variables Initialized.')\n",
    "    last_offset = 0\n",
    "    iteration = 0\n",
    "    inner_epoch_samples = 0\n",
    "    last_epoch = -1\n",
    "    while epoch <= number_of_epochs+1:\n",
    "        # Generate Batches\n",
    "        offset = (iteration * batch_size) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_images[offset :(offset + batch_size), : ]\n",
    "        batch_labels = train_labels[offset : (offset + batch_size), : ]\n",
    "        \n",
    "        # Randomize Within Batch: This slows things down but may be better pratice in general.\n",
    "        randomized_indices = list(range(batch_size))\n",
    "        random.shuffle(randomized_indices)\n",
    "        randomized_batch_data = batch_data[randomized_indices]\n",
    "        randomized_batch_labels = batch_labels[randomized_indices]\n",
    "    \n",
    "        # Run Model\n",
    "        feed_dict = {T : randomized_batch_data, Tlabel : randomized_batch_labels}\n",
    "        _, l, train_predict = session.run([optimizer, loss, train_predictions], feed_dict = feed_dict)\n",
    "        iteration += 1\n",
    "        inner_epoch_samples += batch_size\n",
    "        \n",
    "        # Report\n",
    "        if (epoch % 2) == 0 and epoch > last_epoch:\n",
    "            print('Epoch %d, Iteration: %d' % (epoch, iteration))\n",
    "            print('loss = %f' % l, end = \", \")\n",
    "            train_acc = accuracy(train_predict, randomized_batch_labels)\n",
    "            print('training accuracy = %.1f%%' % train_acc, end = \", \")\n",
    "            valid_acc = accuracy(valid_predictions.eval(), valid_labels)\n",
    "            print('validation accuracy = %.1f%%.' % valid_acc)\n",
    "            last_epoch = epoch\n",
    "        if inner_epoch_samples - train_labels.shape[0] > 0:\n",
    "            inner_epoch_samples = inner_epoch_samples - (train_labels.shape[0]+1)\n",
    "            epoch += 1\n",
    "    test_predict = test_predictions.eval()\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_predict, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (47995, 28, 28, 1)\n",
      "Valid shape: (12005, 28, 28, 1)\n",
      "Test shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def reformat_for_CNN(images):\n",
    "    out_images = images.reshape((-1, 28, 28, 1)).astype(np.float32)\n",
    "    return out_images\n",
    "\n",
    "train_images_CNN = reformat_for_CNN(train_images)\n",
    "valid_images_CNN = reformat_for_CNN(valid_images)\n",
    "test_images_CNN = reformat_for_CNN(test_images)\n",
    "\n",
    "print(\"Train shape: \"+ str(train_images_CNN.shape))\n",
    "print(\"Valid shape: \"+ str(valid_images_CNN.shape))\n",
    "print(\"Test shape: \"+ str(test_images_CNN.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "number_of_nodes = 64\n",
    "number_of_nodes2 =32\n",
    "\n",
    "number_of_channels = 1\n",
    "number_of_labels = 10\n",
    "image_size = 28\n",
    "\n",
    "init_learning_rate = 0.05\n",
    "init_stddev = .1\n",
    "\n",
    "CNN_graph = tf.Graph()\n",
    "\n",
    "with CNN_graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, number_of_channels))\n",
    "    Tlabel = tf.placeholder(tf.float32, shape=(batch_size, number_of_labels))\n",
    "    V = tf.constant(valid_images_CNN)\n",
    "    E = tf.constant(test_images_CNN)\n",
    "  \n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, number_of_channels, depth], stddev=init_stddev))\n",
    "    b1 = tf.Variable(tf.zeros([depth]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=init_stddev))\n",
    "    b2 = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([64, number_of_nodes], stddev=init_stddev))\n",
    "    b3 = tf.Variable(tf.constant(1.0, shape=[number_of_nodes]))\n",
    "    W4 = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes2], stddev=init_stddev))\n",
    "    b4 = tf.Variable(tf.constant(1.0, shape=[number_of_nodes2]))\n",
    "    W5 = tf.Variable(tf.truncated_normal([number_of_nodes2, number_of_labels], stddev=init_stddev))\n",
    "    b5 = tf.Variable(tf.constant(1.0, shape=[number_of_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data, dropout = True):\n",
    "        conv1 = tf.nn.conv2d(data, W1, [1, 2, 2, 1], padding='SAME')\n",
    "        hid1 = tf.nn.leaky_relu(conv1 + b1)\n",
    "        pool1 = tf.nn.max_pool(value = hid1, ksize = [1, 2, 2, 1], strides = [1,2,2,1], padding = 'SAME')\n",
    "        conv2 = tf.nn.conv2d(pool1, W2, [1, 2, 2, 1], padding='SAME')\n",
    "        hid2 = tf.nn.leaky_relu(conv2 + b2)\n",
    "        pool2 = tf.nn.max_pool(value = hid2, ksize = [1, 2, 2, 1], strides = [1,2,2,1], padding = 'SAME')\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        log3 = tf.matmul(reshape, W3) + b3\n",
    "        hid3 = tf.nn.leaky_relu(log3)\n",
    "        if dropout == True:\n",
    "            hid3 = tf.nn.dropout(hid3, keep_prob = .5)\n",
    "        log4 = tf.matmul(hid3, W4) + b4\n",
    "        hid4 = tf.nn.leaky_relu(log4)\n",
    "        if dropout == True:\n",
    "            hid4 = tf.nn.dropout(hid4, keep_prob = .5)\n",
    "        log5 = tf.matmul(hid4, W5) + b5\n",
    "        return log5\n",
    "  \n",
    "    # logits, softmax, crossentropy and loss\n",
    "    L = model(T)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Tlabel, logits=L))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(init_learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_predictions = tf.nn.softmax(model(T, dropout = False))\n",
    "    valid_predictions = tf.nn.softmax(model(V, dropout = False))\n",
    "    test_predictions = tf.nn.softmax(model(E, dropout = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Iteration: 0\n",
      "loss = 3.292928, training accuracy = 0.0%, validation accuracy = 9.7%.\n",
      "Iteration: 1000\n",
      "loss = 0.578934, training accuracy = 68.8%, validation accuracy = 84.4%.\n",
      "Iteration: 2000\n",
      "loss = 0.617082, training accuracy = 93.8%, validation accuracy = 93.9%.\n",
      "Iteration: 3000\n",
      "loss = 0.021760, training accuracy = 100.0%, validation accuracy = 95.0%.\n",
      "Iteration: 4000\n",
      "loss = 0.027779, training accuracy = 100.0%, validation accuracy = 96.0%.\n",
      "Iteration: 5000\n",
      "loss = 0.047241, training accuracy = 100.0%, validation accuracy = 96.2%.\n",
      "Iteration: 6000\n",
      "loss = 0.259676, training accuracy = 93.8%, validation accuracy = 96.4%.\n",
      "Iteration: 7000\n",
      "loss = 0.069597, training accuracy = 100.0%, validation accuracy = 96.2%.\n",
      "Iteration: 8000\n",
      "loss = 0.184429, training accuracy = 100.0%, validation accuracy = 96.9%.\n",
      "Iteration: 9000\n",
      "loss = 0.294641, training accuracy = 87.5%, validation accuracy = 96.7%.\n",
      "Iteration: 10000\n",
      "loss = 0.118383, training accuracy = 93.8%, validation accuracy = 96.3%.\n",
      "Iteration: 11000\n",
      "loss = 0.070614, training accuracy = 100.0%, validation accuracy = 97.4%.\n",
      "Iteration: 12000\n",
      "loss = 0.060742, training accuracy = 100.0%, validation accuracy = 97.1%.\n",
      "Iteration: 13000\n",
      "loss = 0.012518, training accuracy = 100.0%, validation accuracy = 97.3%.\n",
      "Iteration: 14000\n",
      "loss = 0.652468, training accuracy = 93.8%, validation accuracy = 97.4%.\n",
      "Iteration: 15000\n",
      "loss = 0.003363, training accuracy = 100.0%, validation accuracy = 97.3%.\n",
      "Iteration: 16000\n",
      "loss = 0.059231, training accuracy = 100.0%, validation accuracy = 97.0%.\n",
      "Iteration: 17000\n",
      "loss = 0.349828, training accuracy = 93.8%, validation accuracy = 97.2%.\n",
      "Iteration: 18000\n",
      "loss = 0.034435, training accuracy = 100.0%, validation accuracy = 97.3%.\n",
      "Iteration: 19000\n",
      "loss = 0.166014, training accuracy = 93.8%, validation accuracy = 97.5%.\n",
      "Test accuracy: 97.9%\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations = 20000\n",
    "\n",
    "with tf.Session(graph=CNN_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for iteration in range(number_of_iterations):\n",
    "        # Generate batch.\n",
    "        offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_images_CNN[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "        # Run model.\n",
    "        feed_dict = {T : batch_data, Tlabel : batch_labels}\n",
    "        _, l, train_predict = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        if (iteration % 1000) == 0:\n",
    "            print('Iteration: %d' % iteration)\n",
    "            print('loss = %f' % l, end = \", \")\n",
    "            train_acc = accuracy(train_predict, batch_labels)\n",
    "            print('training accuracy = %.1f%%' % train_acc, end = \", \")\n",
    "            valid_acc = accuracy(valid_predictions.eval(), valid_labels)\n",
    "            print('validation accuracy = %.1f%%.' % valid_acc)\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_predictions.eval(), test_labels))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Androids Count Electric Sheep With MNIST Digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scramble_images(train, valid, test):\n",
    "    column_index = list(range(28*28))\n",
    "    random.shuffle(column_index)\n",
    "    return train[:, column_index], valid[:, column_index], test[:, column_index]\n",
    "\n",
    "scrambled_train_images, scrambled_valid_images, scrambled_test_images = scramble_images(train_images, \n",
    "                                                                                        valid_images, \n",
    "                                                                                        test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 40872\n",
      "Label: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEAlJREFUeJzt3X+sVPWZx/HPo0IkgEblygXKr0VCFoxSmaDRZnFTIXRzDTYRLcYNGzelf3DjNsFEwx/WfzYxK6WrwTS5XbHXpNjWoCsSswvBDbbJig4GwS67WyLQwuXHRQhSjZYfz/5xB/cCd75nmHPOnDN836/E3LnzzJzzMPLhzMxzZr7m7gIQn6uKbgBAMQg/ECnCD0SK8AORIvxApAg/ECnCD0SK8AORIvxApK5p5c7GjBnjU6ZMaeUuG7Z9+/Zgfc6cOaXcdtL2Z86cGbzviBEjUu0b5bJv3z4dO3bMGrmtpTm918wWSnpe0tWS/sXdnw3dvlKpeLVabXp/ebrqqvCToHPnzpVy20nb37VrV/C+s2bNSrVvlEulUlG1Wm0o/E0/7TezqyW9KOk7kmZKWmJm4cMMgNJI85p/rqQ97v6Ju/9Z0i8lLcqmLQB5SxP+CZL+OOj3A7XrLmBmy8ysambV/v7+FLsDkKU04R/qdcUlbyC4e4+7V9y90tHRkWJ3ALKUJvwHJE0c9Ps3JPWlawdAq6QJ/weSppvZVDMbLul7kjZk0xaAvDU953f3M2bWLenfNTDqW+vuv8ussxa7++67c9t22lFe0dtvVwcPHqxbmzDhkrenLvDll18G69dee21TPWWht7e3bu3TTz9teDupTvJx97clvZ1mGwCKwem9QKQIPxApwg9EivADkSL8QKQIPxCpVB/pvVxJH+l96623gve///77s26pLeT9kWBcOVrykV4A7Y3wA5Ei/ECkCD8QKcIPRIrwA5Fq6Vd3J0kzyps8eXKwvn///qa3XbSkUV5oFHj77bcH7/vRRx/ltu9G7o9LHT58OFgfOXJk3drlPN4c+YFIEX4gUoQfiBThByJF+IFIEX4gUoQfiFSp5vxptPMcP60iZ+lp9j127Nhg/ciRI01vu511dnY2fd+k8y4uuG3TewHQ1gg/ECnCD0SK8AORIvxApAg/ECnCD0Qq1ZzfzPZJOiXprKQz7l7JoqkiPPbYY8H62rVrm952kZ95L/Pn7ZPm+GXufd68ecH61q1bW9RJ87I4yeev3f1YBtsB0EI87QcilTb8LmmTmW03s2VZNASgNdI+7b/H3fvM7GZJm83sv9393cE3qP2jsEySJk2alHJ3ALKS6sjv7n21n0clvSFp7hC36XH3irtXOjo60uwOQIaaDr+ZjTSz0ecvS1og6eOsGgOQrzRP+8dKesPMzm9nnbv/WyZdAchd0+F3908khb8Uvo2kmeMnKfN313d1dQXrGzduTLX9PXv21K3dcsstwfuWec2ApDn+mjVrgvXu7u4s22kKoz4gUoQfiBThByJF+IFIEX4gUoQfiNQV89XdeQuNlfIe5aUZab3wwgvB++Y9ckoa56VR5hFq0uNa5N+nr3toyV4AlA7hByJF+IFIEX4gUoQfiBThByJF+IFImbu3bmdmwZ2l6eWuu+4K1t97771gvciPhy5YsCBY37RpU277bmfPPfdcsP7kk0/WrSX9/7zzzjuD9W3btgXreXrwwQfr1rZs2aITJ05YI9vhyA9EivADkSL8QKQIPxApwg9EivADkSL8QKRaOuevVCperVZbtr8sTZgwoW7t4MGDue67zEtV5ynWP3calUpF1WqVOT+A+gg/ECnCD0SK8AORIvxApAg/ECnCD0Qq8Xv7zWytpC5JR9391tp1N0r6laQpkvZJesjdT+TXZvHynuWH9PX1FbbvIqWd48+bN69uLWmJ7bTa4RyFRo78P5e08KLrnpK0xd2nS9pS+x1AG0kMv7u/K+n4RVcvktRbu9wr6YGM+wKQs2Zf849190OSVPt5c3YtAWiF3N/wM7NlZlY1s2p/f3/euwPQoGbDf8TMxklS7efRejd09x53r7h7paOjo8ndAchas+HfIGlp7fJSSW9m0w6AVkkMv5m9Kuk/Jc0wswNm9veSnpU038x+L2l+7XcAbSRxzu/uS+qUvp1xL6ijs7Oz6Bba0rRp0wrbdxnm+Ek4ww+IFOEHIkX4gUgRfiBShB+IFOEHIpU46iuT5cuX1629+OKLue775MmTdWvXX399rvtOY82aNcF6d3d3sP7+++8H63Pnzr3sns7r6uoK1hcuvPjDpBdK6n3VqlWX3VOjivzI7owZM+rW9u/f3/B2OPIDkSL8QKQIPxApwg9EivADkSL8QKQIPxApluiumTp1arC+YsWKurWkefPevXtT7budhebhSbPww4cPB+t5ftQ57zl+msclhCW6ASQi/ECkCD8QKcIPRIrwA5Ei/ECkCD8QqZbO+Ts7O/3RRx+tW0/6/HVes9FGHD9+8Vql/2/UqFHB+w4fPjzrdoAhMecHkIjwA5Ei/ECkCD8QKcIPRIrwA5Ei/ECkEuf8ZrZWUpeko+5+a+26ZyR9X1J/7WYr3f3tpJ2NGDHCp0yZUre+e/fuhppGeRT5/fW4VNZz/p9LGmr1hJ+4++zaf4nBB1AuieF393cl1T+9DUBbSvOav9vMdprZWjO7IbOOALREs+H/qaRpkmZLOiTpx/VuaGbLzKxqZtUzZ840uTsAWWsq/O5+xN3Puvs5ST+TVHe1RnfvcfeKu1euuaat1gUFrmhNhd/Mxg369buSPs6mHQCtkngoNrNXJd0raYyZHZD0I0n3mtlsSS5pn6Qf5NgjgBwkht/dlwxx9UvN7GzWrFnK63v7k+bNSco8jy7zLL3MjxvCOMMPiBThByJF+IFIEX4gUoQfiBThByLVVqfczZs3r27tSh45pfmz9fX1Bevjx48P1pOWD09afrxdJY1Xv/rqq2B92LBhWbaTC478QKQIPxApwg9EivADkSL8QKQIPxApwg9EqlRz/jQfXZ09e3bwvtOnTw/W169f3/S+00r6c588eTJYHz16dN1a0hw/yZU6x09yJZ83ch5HfiBShB+IFOEHIkX4gUgRfiBShB+IFOEHItXSOf/27duDM+2XX3656W3v2LGj6ftKyZ97z1OZZ8p5fm34ddddF6zfdNNNwfqVeg5Cq76qnSM/ECnCD0SK8AORIvxApAg/ECnCD0SK8AORSpzzm9lESa9I6pR0TlKPuz9vZjdK+pWkKZL2SXrI3U+EtjVnzpxUS3Q//PDDdWuvv/568L6nT58O1tN+7r2skmbhSd/LnzRTTjOT/uyzz4L3LdITTzwRrK9atSq3fbfqvI9GjvxnJK1w97+UdJek5WY2U9JTkra4+3RJW2q/A2gTieF390Pu/mHt8ilJuyVNkLRIUm/tZr2SHsirSQDZu6zX/GY2RdI3JW2TNNbdD0kD/0BIujnr5gDkp+Hwm9koSesl/dDdG36xZmbLzKxqZtX+/v5megSQg4bCb2bDNBD8X7j7+XfWjpjZuFp9nKSjQ93X3XvcveLulY6Ojix6BpCBxPCbmUl6SdJud189qLRB0tLa5aWS3sy+PQB5MXcP38DsW5J+I2mXBkZ9krRSA6/7fy1pkqQ/SFrs7sdD26pUKp5m1Bfy+eefB+sjR47MZb+N2Lx5c7A+f/78VNtfsGBB3dqmTZtSbRvtpVKpqFqtWiO3TZzzu/tvJdXb2LcvpzEA5cEZfkCkCD8QKcIPRIrwA5Ei/ECkCD8QqcQ5f5bynPMjH+vWrQvWH3nkkRZ10l7eeeedYP2+++6rW0v6SG9oSfYvvvhCZ8+ebWjOz5EfiBThByJF+IFIEX4gUoQfiBThByJF+IFItXSJbrSfpDl+V1dXsL5x48Ys28lM2mWw16xZE6x3d3en2n7IqVOn6tYqlUrD2+HID0SK8AORIvxApAg/ECnCD0SK8AORIvxApEo151+8eHGw/tprr+W277Rz3zzt3LkzWL/tttua3vbjjz8erK9evTpYT5rj9/b21q0tXbq0bi1vaf9/Js3x8/T000/XrfX19TW8HY78QKQIPxApwg9EivADkSL8QKQIPxApwg9EKvF7+81soqRXJHVKOiepx92fN7NnJH1fUn/tpivd/e3QtkaPHu133HFH3frWrVuDvYRm8UXO4dPq6ekJ1pctW9aiTlqryHMrynxeRxqVSkXVarWh7+1v5CSfM5JWuPuHZjZa0nYz21yr/cTdVzXbKIDiJIbf3Q9JOlS7fMrMdkuakHdjAPJ1Wa/5zWyKpG9K2la7qtvMdprZWjO7oc59lplZ1cyqp0+fTtUsgOw0HH4zGyVpvaQfuvtnkn4qaZqk2Rp4ZvDjoe7n7j3uXnH3yrBhwzJoGUAWGgq/mQ3TQPB/4e6vS5K7H3H3s+5+TtLPJM3Nr00AWUsMv5mZpJck7Xb31YOuHzfoZt+V9HH27QHISyPv9t8j6W8l7TKzHbXrVkpaYmazJbmkfZJ+kLShGTNmJI7z8pJ2tJPnmDHtKC/0XkqZX2oVOU6bPHlyYfsui0be7f+tpKHmhsGZPoBy4ww/IFKEH4gU4QciRfiBSBF+IFKEH4hUqb66O0mauXDamXKa+/f39wfrHR0dwfrhw4eD9fHjx9et5T1LX7duXbCetMR3Ufbu3Vt0C4XjyA9EivADkSL8QKQIPxApwg9EivADkSL8QKQSv7o7052Z9UvaP+iqMZKOtayBy1PW3sral0Rvzcqyt8nuHj5xpKal4b9k52ZVd68U1kBAWXsra18SvTWrqN542g9EivADkSo6/OF1qopV1t7K2pdEb80qpLdCX/MDKE7RR34ABSkk/Ga20Mz+x8z2mNlTRfRQj5ntM7NdZrbDzKoF97LWzI6a2ceDrrvRzDab2e9rP4dcJq2g3p4xs4O1x26Hmf1NQb1NNLP/MLPdZvY7M/uH2vWFPnaBvgp53Fr+tN/Mrpb0v5LmSzog6QNJS9z9v1raSB1mtk9Sxd0Lnwmb2V9J+pOkV9z91tp1/yTpuLs/W/uH8wZ3f7IkvT0j6U9Fr9xcW1Bm3OCVpSU9IOnvVOBjF+jrIRXwuBVx5J8raY+7f+Luf5b0S0mLCuij9Nz9XUnHL7p6kaTe2uVeDfzlabk6vZWCux9y9w9rl09JOr+ydKGPXaCvQhQR/gmS/jjo9wMq15LfLmmTmW03s3RL6eRjbG3Z9PPLp99ccD8XS1y5uZUuWlm6NI9dMyteZ62I8A+1+k+ZRg73uPsdkr4jaXnt6S0a09DKza0yxMrSpdDsitdZKyL8ByRNHPT7NyT1FdDHkNy9r/bzqKQ3VL7Vh4+cXyS19vNowf18rUwrNw+1srRK8NiVacXrIsL/gaTpZjbVzIZL+p6kDQX0cQkzG1l7I0ZmNlLSApVv9eENkpbWLi+V9GaBvVygLCs311tZWgU/dmVb8bqQk3xqo4x/lnS1pLXu/o8tb2IIZvYXGjjaSwPfbLyuyN7M7FVJ92rgU19HJP1I0r9K+rWkSZL+IGmxu7f8jbc6vd2rgaeuX6/cfP41dot7+5ak30jaJen81xev1MDr68Ieu0BfS1TA48YZfkCkOMMPiBThByJF+IFIEX4gUoQfiBThByJF+IFIEX4gUv8HFgk0I8EI2vgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "deformatted_scrambled_train_images = scrambled_train_images.reshape(-1,28,28)\n",
    "show_random_image(deformatted_scrambled_train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Regression on Scrambled Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_regression_graph = tf.Graph()\n",
    "batch_size = 1000\n",
    "\n",
    "with stochastic_regression_graph.as_default():\n",
    "    # Placeholders.\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_size, 784)) # Trains batches of 100.\n",
    "    Tlabel = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    # Constants.\n",
    "    V = tf.constant(scrambled_valid_images)\n",
    "    E = tf.constant(scrambled_test_images)\n",
    "    # Variables.\n",
    "    W = tf.Variable(tf.truncated_normal([784, 10])) # Randomly initialized.\n",
    "    b = tf.Variable(tf.zeros([10])) # Initialized at origin. \n",
    "    # Model.\n",
    "    L = tf.matmul(T,W)+b\n",
    "    # softmax, crossentropy and loss\n",
    "    smce = tf.nn.softmax_cross_entropy_with_logits_v2(labels = Tlabel, logits = L)\n",
    "    loss = tf.reduce_mean(smce)\n",
    "    # Optimize\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    # Monitoring outputs.\n",
    "    scrambled_train_predictions = tf.nn.softmax(L)\n",
    "    scrambled_valid_predictions = tf.nn.softmax(tf.matmul(V,W)+b)\n",
    "    scrambled_test_predictions = tf.nn.softmax(tf.matmul(E,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized.\n",
      "Epoch 0, Iteration: 1\n",
      "loss = 22.711788, training accuracy = 7.8%, validation accuracy = 8.9%.\n",
      "Epoch 100, Iteration: 4801\n",
      "loss = 0.302894, training accuracy = 91.3%, validation accuracy = 90.4%.\n",
      "Epoch 200, Iteration: 9601\n",
      "loss = 0.292581, training accuracy = 91.5%, validation accuracy = 90.8%.\n",
      "Epoch 300, Iteration: 14400\n",
      "loss = 0.210696, training accuracy = 93.1%, validation accuracy = 91.2%.\n",
      "Epoch 400, Iteration: 19200\n",
      "loss = 0.268976, training accuracy = 92.9%, validation accuracy = 91.4%.\n",
      "Epoch 500, Iteration: 23999\n",
      "loss = 0.231109, training accuracy = 94.1%, validation accuracy = 91.2%.\n",
      "Epoch 600, Iteration: 28799\n",
      "loss = 0.203045, training accuracy = 94.3%, validation accuracy = 91.5%.\n",
      "Epoch 700, Iteration: 33599\n",
      "loss = 0.232386, training accuracy = 94.4%, validation accuracy = 91.7%.\n",
      "Epoch 800, Iteration: 38398\n",
      "loss = 0.237743, training accuracy = 93.4%, validation accuracy = 91.8%.\n",
      "Epoch 900, Iteration: 43198\n",
      "loss = 0.204907, training accuracy = 94.2%, validation accuracy = 91.7%.\n",
      "Epoch 1000, Iteration: 47997\n",
      "loss = 0.209195, training accuracy = 92.9%, validation accuracy = 91.5%.\n",
      "Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 1000\n",
    "\n",
    "with tf.Session(graph = stochastic_regression_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    epoch = 0\n",
    "    print('Variables Initialized.')\n",
    "    last_offset = 0\n",
    "    iteration = 0\n",
    "    inner_epoch_samples = 0\n",
    "    last_epoch = -1\n",
    "    while epoch <= number_of_epochs+1:\n",
    "        # Generate Batches\n",
    "        offset = (iteration * batch_size) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = scrambled_train_images[offset :(offset + batch_size), : ]\n",
    "        batch_labels = train_labels[offset : (offset + batch_size), : ]\n",
    "        \n",
    "        # Randomize Within Batch: This slows things down but may be better pratice.\n",
    "        randomized_indices = list(range(batch_size))\n",
    "        random.shuffle(randomized_indices)\n",
    "        randomized_batch_data = batch_data[randomized_indices]\n",
    "        randomized_batch_labels = batch_labels[randomized_indices]\n",
    "        \n",
    "        # Run Model\n",
    "        feed_dict = {T : randomized_batch_data, Tlabel : randomized_batch_labels}\n",
    "        _, l, train_predict = session.run([optimizer, loss, scrambled_train_predictions], feed_dict = feed_dict)\n",
    "        iteration += 1\n",
    "        inner_epoch_samples += batch_size\n",
    "        \n",
    "        # Report\n",
    "        if (epoch % 100) == 0 and epoch > last_epoch:\n",
    "            print('Epoch %d, Iteration: %d' % (epoch, iteration))\n",
    "            print('loss = %f' % l, end = \", \")\n",
    "            train_acc = accuracy(train_predict, randomized_batch_labels)\n",
    "            print('training accuracy = %.1f%%' % train_acc, end = \", \")\n",
    "            valid_acc = accuracy(scrambled_valid_predictions.eval(), valid_labels)\n",
    "            print('validation accuracy = %.1f%%.' % valid_acc)\n",
    "            last_epoch = epoch\n",
    "        if inner_epoch_samples - train_labels.shape[0] > 0:\n",
    "            inner_epoch_samples = inner_epoch_samples - (train_labels.shape[0]+1)\n",
    "            epoch += 1\n",
    "    scrambled_test_predict = scrambled_test_predictions.eval()\n",
    "    print('Test accuracy: %.1f%%' % accuracy(scrambled_test_predict, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network, 4 Hidden Layers on Scrambled Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "number_of_nodes = 1024 \n",
    "learning_rate = .025\n",
    "init_stddev = .05\n",
    "\n",
    "FCNN_4HL_graph = tf.Graph()\n",
    "\n",
    "with FCNN_4HL_graph.as_default():\n",
    "    # Placeholder.\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_size, 784))\n",
    "    Tlabel = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    # Constant.\n",
    "    V = tf.constant(scrambled_valid_images)\n",
    "    E = tf.constant(scrambled_test_images)\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.random_normal([784, number_of_nodes], stddev = init_stddev))\n",
    "    b1 = tf.Variable(tf.zeros([number_of_nodes]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([number_of_nodes, int(number_of_nodes/2)], stddev = init_stddev))\n",
    "    b2 = tf.Variable(tf.zeros([int(number_of_nodes/2)])) \n",
    "    W3 = tf.Variable(tf.truncated_normal([int(number_of_nodes/2), int(number_of_nodes/4)], stddev = init_stddev))\n",
    "    b3 = tf.Variable(tf.zeros([int(number_of_nodes/4)])) \n",
    "    W4 = tf.Variable(tf.truncated_normal([int(number_of_nodes/4), int(number_of_nodes/8)], stddev = init_stddev))\n",
    "    b4 = tf.Variable(tf.zeros([int(number_of_nodes/8)])) \n",
    "    W5 = tf.Variable(tf.truncated_normal([int(number_of_nodes/8), 10], stddev = init_stddev))\n",
    "    b5 = tf.Variable(tf.zeros([10])) \n",
    "    # Model\n",
    "    def FCNN_4HL_model(data, dropout = True):\n",
    "        log1 = tf.matmul(data, W1) + b1\n",
    "        hid1 = tf.nn.leaky_relu(log1)\n",
    "        if dropout:\n",
    "            hid1 = tf.nn.dropout(hid1, keep_prob = .5)\n",
    "        log2 = tf.matmul(hid1, W2)+b2\n",
    "        hid2 = tf.nn.leaky_relu(log2)\n",
    "        if dropout:\n",
    "            hid2 = tf.nn.dropout(hid2, keep_prob = .5)\n",
    "        log3 = tf.matmul(hid2, W3)+b3\n",
    "        hid3 = tf.nn.leaky_relu(log3)\n",
    "        if dropout:\n",
    "            hid3 = tf.nn.dropout(hid3, keep_prob = .5)\n",
    "        log4 = tf.matmul(hid3, W4)+b4\n",
    "        hid4 = tf.nn.leaky_relu(log4)\n",
    "        if dropout:\n",
    "            hid4 = tf.nn.dropout(hid4, keep_prob = .5)\n",
    "        log5 = tf.matmul(hid4, W5)+b5\n",
    "        return log5\n",
    "    \n",
    "    \n",
    "    # logits, softmax, crossentropy and loss\n",
    "    L = FCNN_4HL_model(T)\n",
    "    smce = tf.nn.softmax_cross_entropy_with_logits_v2(labels = Tlabel, logits = L)\n",
    "    loss = tf.reduce_mean(smce)\n",
    "    # Optimize\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    # Monitoring outputs.\n",
    "    scrambled_train_predictions = tf.nn.softmax(FCNN_4HL_model(T, dropout = False))\n",
    "    scrambled_valid_predictions = tf.nn.softmax(FCNN_4HL_model(V, dropout = False))\n",
    "    scrambled_test_predictions = tf.nn.softmax(FCNN_4HL_model(E, dropout = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized.\n",
      "Epoch 0, Iteration: 1\n",
      "loss = 2.358292, training accuracy = 11.0%, validation accuracy = 12.8%.\n",
      "Epoch 2, Iteration: 961\n",
      "loss = 0.559024, training accuracy = 86.0%, validation accuracy = 86.8%.\n",
      "Epoch 4, Iteration: 1921\n",
      "loss = 0.456431, training accuracy = 89.0%, validation accuracy = 91.0%.\n",
      "Epoch 6, Iteration: 2881\n",
      "loss = 0.470253, training accuracy = 89.0%, validation accuracy = 92.4%.\n",
      "Epoch 8, Iteration: 3841\n",
      "loss = 0.333921, training accuracy = 88.0%, validation accuracy = 93.5%.\n",
      "Epoch 10, Iteration: 4801\n",
      "loss = 0.321924, training accuracy = 90.0%, validation accuracy = 94.4%.\n",
      "Epoch 12, Iteration: 5761\n",
      "loss = 0.254863, training accuracy = 96.0%, validation accuracy = 94.8%.\n",
      "Epoch 14, Iteration: 6721\n",
      "loss = 0.237653, training accuracy = 95.0%, validation accuracy = 95.2%.\n",
      "Epoch 16, Iteration: 7681\n",
      "loss = 0.175100, training accuracy = 96.0%, validation accuracy = 95.6%.\n",
      "Epoch 18, Iteration: 8641\n",
      "loss = 0.130864, training accuracy = 96.0%, validation accuracy = 95.8%.\n",
      "Epoch 20, Iteration: 9601\n",
      "loss = 0.271612, training accuracy = 95.0%, validation accuracy = 96.0%.\n",
      "Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 20\n",
    "\n",
    "with tf.Session(graph = FCNN_4HL_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    epoch = 0\n",
    "    print('Variables Initialized.')\n",
    "    last_offset = 0\n",
    "    iteration = 0\n",
    "    inner_epoch_samples = 0\n",
    "    last_epoch = -1\n",
    "    while epoch <= number_of_epochs+1:\n",
    "        # Generate Batches\n",
    "        offset = (iteration * batch_size) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = scrambled_train_images[offset :(offset + batch_size), : ]\n",
    "        batch_labels = train_labels[offset : (offset + batch_size), : ]\n",
    "        \n",
    "        # Randomize Within Batch: This slows things down but may be better pratice in general.\n",
    "        randomized_indices = list(range(batch_size))\n",
    "        random.shuffle(randomized_indices)\n",
    "        randomized_batch_data = batch_data[randomized_indices]\n",
    "        randomized_batch_labels = batch_labels[randomized_indices]\n",
    "    \n",
    "        # Run Model\n",
    "        feed_dict = {T : randomized_batch_data, Tlabel : randomized_batch_labels}\n",
    "        _, l, train_predict = session.run([optimizer, loss, scrambled_train_predictions], feed_dict = feed_dict)\n",
    "        iteration += 1\n",
    "        inner_epoch_samples += batch_size\n",
    "        \n",
    "        # Report\n",
    "        if (epoch % 2) == 0 and epoch > last_epoch:\n",
    "            print('Epoch %d, Iteration: %d' % (epoch, iteration))\n",
    "            print('loss = %f' % l, end = \", \")\n",
    "            train_acc = accuracy(train_predict, randomized_batch_labels)\n",
    "            print('training accuracy = %.1f%%' % train_acc, end = \", \")\n",
    "            valid_acc = accuracy(scrambled_valid_predictions.eval(), valid_labels)\n",
    "            print('validation accuracy = %.1f%%.' % valid_acc)\n",
    "            last_epoch = epoch\n",
    "        if inner_epoch_samples - train_labels.shape[0] > 0:\n",
    "            inner_epoch_samples = inner_epoch_samples - (train_labels.shape[0]+1)\n",
    "            epoch += 1\n",
    "    test_predict = scrambled_test_predictions.eval()\n",
    "    print('Test accuracy: %.1f%%' % accuracy(scrambled_test_predict, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (47995, 28, 28, 1)\n",
      "Valid shape: (12005, 28, 28, 1)\n",
      "Test shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def reformat_for_CNN(images):\n",
    "    out_images = images.reshape((-1, 28, 28, 1)).astype(np.float32)\n",
    "    return out_images\n",
    "\n",
    "scrambled_train_images_CNN = reformat_for_CNN(scrambled_train_images)\n",
    "scrambled_valid_images_CNN = reformat_for_CNN(scrambled_valid_images)\n",
    "scrambled_test_images_CNN = reformat_for_CNN(scrambled_test_images)\n",
    "\n",
    "print(\"Train shape: \"+ str(scrambled_train_images_CNN.shape))\n",
    "print(\"Valid shape: \"+ str(scrambled_valid_images_CNN.shape))\n",
    "print(\"Test shape: \"+ str(scrambled_test_images_CNN.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "number_of_nodes = 64\n",
    "number_of_nodes2 =32\n",
    "\n",
    "number_of_channels = 1\n",
    "number_of_labels = 10\n",
    "image_size = 28\n",
    "\n",
    "init_learning_rate = 0.05\n",
    "init_stddev = .1\n",
    "\n",
    "CNN_graph = tf.Graph()\n",
    "\n",
    "with CNN_graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, number_of_channels))\n",
    "    Tlabel = tf.placeholder(tf.float32, shape=(batch_size, number_of_labels))\n",
    "    V = tf.constant(scrambled_valid_images_CNN)\n",
    "    E = tf.constant(scrambled_test_images_CNN)\n",
    "  \n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, number_of_channels, depth], stddev=init_stddev))\n",
    "    b1 = tf.Variable(tf.zeros([depth]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=init_stddev))\n",
    "    b2 = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([64, number_of_nodes], stddev=init_stddev))\n",
    "    b3 = tf.Variable(tf.constant(1.0, shape=[number_of_nodes]))\n",
    "    W4 = tf.Variable(tf.truncated_normal([number_of_nodes, number_of_nodes2], stddev=init_stddev))\n",
    "    b4 = tf.Variable(tf.constant(1.0, shape=[number_of_nodes2]))\n",
    "    W5 = tf.Variable(tf.truncated_normal([number_of_nodes2, number_of_labels], stddev=init_stddev))\n",
    "    b5 = tf.Variable(tf.constant(1.0, shape=[number_of_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data, dropout = True):\n",
    "        conv1 = tf.nn.conv2d(data, W1, [1, 2, 2, 1], padding='SAME')\n",
    "        hid1 = tf.nn.leaky_relu(conv1 + b1)\n",
    "        pool1 = tf.nn.max_pool(value = hid1, ksize = [1, 2, 2, 1], strides = [1,2,2,1], padding = 'SAME')\n",
    "        conv2 = tf.nn.conv2d(pool1, W2, [1, 2, 2, 1], padding='SAME')\n",
    "        hid2 = tf.nn.leaky_relu(conv2 + b2)\n",
    "        pool2 = tf.nn.max_pool(value = hid2, ksize = [1, 2, 2, 1], strides = [1,2,2,1], padding = 'SAME')\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        log3 = tf.matmul(reshape, W3) + b3\n",
    "        hid3 = tf.nn.leaky_relu(log3)\n",
    "        if dropout == True:\n",
    "            hid3 = tf.nn.dropout(hid3, keep_prob = .5)\n",
    "        log4 = tf.matmul(hid3, W4) + b4\n",
    "        hid4 = tf.nn.leaky_relu(log4)\n",
    "        if dropout == True:\n",
    "            hid4 = tf.nn.dropout(hid4, keep_prob = .5)\n",
    "        log5 = tf.matmul(hid4, W5) + b5\n",
    "        return log5\n",
    "  \n",
    "    # logits, softmax, crossentropy and loss\n",
    "    L = model(T)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Tlabel, logits=L))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(init_learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    scrambled_train_predictions = tf.nn.softmax(model(T, dropout = False))\n",
    "    scrambled_valid_predictions = tf.nn.softmax(model(V, dropout = False))\n",
    "    scrambled_test_predictions = tf.nn.softmax(model(E, dropout = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Iteration: 0\n",
      "loss = 2.433147, training accuracy = 25.0%, validation accuracy = 9.9%.\n",
      "Iteration: 1000\n",
      "loss = 1.266197, training accuracy = 56.2%, validation accuracy = 40.2%.\n",
      "Iteration: 2000\n",
      "loss = 0.727661, training accuracy = 56.2%, validation accuracy = 78.3%.\n",
      "Iteration: 3000\n",
      "loss = 0.408943, training accuracy = 100.0%, validation accuracy = 86.3%.\n",
      "Iteration: 4000\n",
      "loss = 1.211791, training accuracy = 75.0%, validation accuracy = 88.2%.\n",
      "Iteration: 5000\n",
      "loss = 1.054869, training accuracy = 81.2%, validation accuracy = 89.7%.\n",
      "Iteration: 6000\n",
      "loss = 0.664420, training accuracy = 81.2%, validation accuracy = 90.7%.\n",
      "Iteration: 7000\n",
      "loss = 0.352289, training accuracy = 93.8%, validation accuracy = 91.4%.\n",
      "Iteration: 8000\n",
      "loss = 0.443306, training accuracy = 93.8%, validation accuracy = 92.3%.\n",
      "Iteration: 9000\n",
      "loss = 0.169416, training accuracy = 100.0%, validation accuracy = 93.2%.\n",
      "Iteration: 10000\n",
      "loss = 0.406815, training accuracy = 93.8%, validation accuracy = 92.9%.\n",
      "Iteration: 11000\n",
      "loss = 0.101683, training accuracy = 100.0%, validation accuracy = 93.3%.\n",
      "Iteration: 12000\n",
      "loss = 0.271405, training accuracy = 87.5%, validation accuracy = 93.6%.\n",
      "Iteration: 13000\n",
      "loss = 0.242295, training accuracy = 93.8%, validation accuracy = 93.6%.\n",
      "Iteration: 14000\n",
      "loss = 0.146703, training accuracy = 93.8%, validation accuracy = 93.9%.\n",
      "Iteration: 15000\n",
      "loss = 0.278105, training accuracy = 93.8%, validation accuracy = 94.1%.\n",
      "Iteration: 16000\n",
      "loss = 0.274756, training accuracy = 100.0%, validation accuracy = 94.2%.\n",
      "Iteration: 17000\n",
      "loss = 0.019892, training accuracy = 100.0%, validation accuracy = 94.2%.\n",
      "Iteration: 18000\n",
      "loss = 0.319693, training accuracy = 87.5%, validation accuracy = 94.5%.\n",
      "Iteration: 19000\n",
      "loss = 0.084305, training accuracy = 100.0%, validation accuracy = 94.1%.\n",
      "Test accuracy: 94.3%\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations = 20000\n",
    "\n",
    "with tf.Session(graph=CNN_graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for iteration in range(number_of_iterations):\n",
    "        # Generate batch.\n",
    "        offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = scrambled_train_images_CNN[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "        # Run model.\n",
    "        feed_dict = {T : batch_data, Tlabel : batch_labels}\n",
    "        _, l, train_predict = session.run([optimizer, loss, scrambled_train_predictions], feed_dict=feed_dict)\n",
    "        if (iteration % 1000) == 0:\n",
    "            print('Iteration: %d' % iteration)\n",
    "            print('loss = %f' % l, end = \", \")\n",
    "            train_acc = accuracy(train_predict, batch_labels)\n",
    "            print('training accuracy = %.1f%%' % train_acc, end = \", \")\n",
    "            valid_acc = accuracy(scrambled_valid_predictions.eval(), valid_labels)\n",
    "            print('validation accuracy = %.1f%%.' % valid_acc)\n",
    "    print('Test accuracy: %.1f%%' % accuracy(scrambled_test_predictions.eval(), test_labels))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: They may count sheep thusly...\n",
    "\n",
    "But having only a mild drop in performance on the scrambled images suggests that these architectures have yet to capture the essence of visual data.  Its still mysterious to me why this drop in performance occurred at all; there are two feasible explanations (optimization difficulty and batch training) that explain the degredataion in a way that doesn't defeat the suggestion that vision has yet to be captured theoretically.\n",
    "\n",
    "It would be interesting to try this experiment on some of the deeper, higher-performing models referenced at the bottom of [this page](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
